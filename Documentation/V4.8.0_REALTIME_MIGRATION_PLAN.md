# V4.8.0 Real-Time Migration Plan

**Version:** 4.8.0  
**Date:** 2026-01-21  
**Status:** Planning (Not Implemented)  
**Goal:** Replace all polling with Supabase real-time subscriptions to reduce Vercel invocations by ~99%

---

## Executive Summary

Currently, the Room page polls every 2.5 seconds, generating 24-44 API calls per minute per user. The TV page already uses real-time successfully. This migration will extend real-time to the Room page with graceful degradation, targeting 90-99% API call reduction.

**Current State:**
- Room page: 24-44 API calls/minute (polling)
- TV page: Real-time with 2.5s polling fallback
- ApprovalQueue: 6-20 API calls/minute (backup + fallback polling)

**Target State:**
- Room page: ~1 API call per session (real-time) OR 8 calls/min (fallback)
- TV page: Same real-time, optimized fallback (7.5s instead of 2.5s)
- ApprovalQueue: ~1 API call per session OR 8 calls/min (remove backup polling)

**Strategy:**
1. **Primary:** Real-time with retry logic (5 attempts)
2. **Fallback:** Slower polling (7.5s) for service continuity
3. **Expected:** 90% users on real-time, 10% on fallback
4. **Result:** ~95% overall reduction in API calls

---

## ⚠️ CRITICAL POLICY: REAL-TIME FIRST, GRACEFUL DEGRADATION

**This migration prioritizes real-time with intelligent fallback. Key principles:**

1. **NO CACHING:**
   - All API calls use `cache: 'no-store'`
   - All headers include `'Cache-Control': 'no-cache'`
   - Already implemented in `src/lib/api.ts` - verify before proceeding

2. **REAL-TIME IS PRIMARY (MUST):**
   - ✅ Real-time is the primary connection method
   - ✅ Use retry logic (5 attempts, 5 second intervals)
   - ✅ After 5 failed retries → graceful degradation to polling
   - **Why?** Users need a working service, even if degraded

3. **GRACEFUL DEGRADATION TO POLLING:**
   - If real-time fails after 5 retries → fallback to **slower polling**
   - **Fallback interval: 7.5 seconds** (compromise between 5-10s)
   - Current: 2.5s polling = 24 calls/min
   - Fallback: 7.5s polling = 8 calls/min
   - **Reduction: 67%** even in degraded mode
   - **Best case (real-time works): 99% reduction**

4. **METRIC PROTECTION:**
   - Target: 90-99% reduction in Vercel invocations
   - Real-time working: ~99% reduction
   - Real-time failed (polling fallback): ~67% reduction
   - Monitor Vercel dashboard first 24 hours

5. **USER EXPERIENCE PRIORITY:**
   - Real-time: Instant updates (<500ms)
   - Fallback: Slower updates (7.5s) but service continues
   - Show connection status to user (optional enhancement)

---

## Current State Analysis (Verified)

### ✅ Already Using Real-Time

#### 1. TV Page (`src/app/tv/page.tsx`)
- **Lines 280-335**: Real-time subscription implementation
- **Subscribes to:**
  - `kara_rooms` table (UPDATE events, line 291-305)
  - `kara_queue` table (all events, line 307-320)
- **Fallback:** 2.5s polling if real-time fails (line 344) - ⚠️ **TO BE REMOVED**
- **Status:** ✅ Working reliably in production
- **Action Required:** Update TV page to use retry logic instead of polling fallback

#### 2. ApprovalQueue Component (`src/components/ApprovalQueue.tsx`)
- **Lines 280-310**: Real-time subscription for participants
- **Subscribes to:**
  - `kara_room_participants` table (all events)
- **Fallback:** 3s polling if real-time fails (line 333)
- **Backup:** 10s polling even when real-time works (line 322) - **TO BE REMOVED**

### ❌ Currently Polling (Needs Migration)

#### Room Page (`src/app/room/[code]/page.tsx`)

**Polling #1: Room State (Line 612)**
```typescript
pollingIntervalRef.current = setInterval(() => {
  refreshRoomState(currentRoomId); // Calls api.getRoomState()
}, 2500); // Every 2.5 seconds
```
- **API Endpoint:** `/api/rooms/[roomId]/state`
- **Fetches:** room metadata, queue, upNext, currentSong
- **Impact:** 24 API calls/minute per active user

**Polling #2: Approval Status (Line 645)**
```typescript
approvalCheckIntervalRef.current = setInterval(async () => {
  const status = await checkApprovalStatus(roomId, userId); // Calls api.getUserStatus()
}, 3000); // Every 3 seconds
```
- **API Endpoint:** `/api/rooms/[roomId]/user-status`
- **Fetches:** User approval status (pending users only)
- **Impact:** 20 API calls/minute per pending user

---

## Supabase Capacity (Verified)

**Current Usage (from dashboard screenshot):**
- Realtime Concurrent Peak: **3/500** connections (<1%)
- Realtime Messages: **690/5,000,000** (<1%)
- **Result:** ✅ Plenty of capacity for migration

---

## Implementation Plan

### Phase 1: Room Page Real-Time Migration

#### Task 1.0: Verify NO CACHING (Critical)

**IMPORTANT:** Real-time updates MUST NOT use any caching.

**Verify in `src/lib/api.ts` (line 53-64):**
```typescript
async getRoomState(roomId: string): Promise<RoomState> {
  // ✅ CORRECT: No caching, fresh data always
  const res = await fetch(`${API_BASE}/rooms/${roomId}/state?t=${Date.now()}`, {
    cache: 'no-store',
    headers: {
      'Cache-Control': 'no-cache, no-store, must-revalidate',
      'Pragma': 'no-cache'
    }
  });
  if (!res.ok) throw new Error('Failed to get room state');
  return res.json();
}
```

**Verify this is already implemented** - if not, update before proceeding.

---

#### Task 1.1: Add Real-Time Room/Queue Subscription

**File:** `src/app/room/[code]/page.tsx`

**Action:** Copy proven pattern from TV page (lines 280-335)

**CRITICAL:** Do NOT implement continuous polling fallback - use retry logic instead

**Add State:**
```typescript
const [useRealtime, setUseRealtime] = useState(true);
const realtimeChannelRef = useRef<RealtimeChannel | null>(null);
```

**Add Callback:**
```typescript
const subscribeToRoom = useCallback((roomId: string) => {
  if (realtimeChannelRef.current) {
    supabase.removeChannel(realtimeChannelRef.current);
    realtimeChannelRef.current = null;
  }

  const channel = supabase
    .channel(`room-${roomId}`)
    // Subscribe to room metadata changes
    .on(
      'postgres_changes',
      {
        event: 'UPDATE',
        schema: 'public',
        table: 'kara_rooms',
        filter: `id=eq.${roomId}`
      },
      (payload) => {
        console.log('[room] Realtime: Room updated', payload);
        refreshRoomState(roomId);
      }
    )
    // Subscribe to queue changes
    .on(
      'postgres_changes',
      {
        event: '*', // INSERT, UPDATE, DELETE
        schema: 'public',
        table: 'kara_queue',
        filter: `room_id=eq.${roomId}`
      },
      (payload) => {
        console.log('[room] Realtime: Queue updated', payload.eventType);
        refreshRoomState(roomId);
      }
    )
    .subscribe((status) => {
      console.log('[room] Realtime subscription status:', status);
      
      if (status === 'SUBSCRIBED') {
        console.log('[room] ✅ Realtime connected');
        // Stop any fallback polling if it was running
        if (pollingIntervalRef.current) {
          clearInterval(pollingIntervalRef.current);
          pollingIntervalRef.current = null;
        }
      } else if (status === 'CHANNEL_ERROR' || status === 'TIMED_OUT') {
        console.error('[room] ❌ Realtime connection failed');
        // DO NOT start continuous polling - only retry real-time
        // This prevents increased Vercel invocations
        attemptRealtimeReconnect(roomId);
      }
    });
  
  realtimeChannelRef.current = channel;
}, [refreshRoomState, startPolling]);
```

**Integration:**
- Call `subscribeToRoom(roomId)` after initial room load
- Add cleanup in useEffect return

**Add Reconnection Strategy:**
```typescript
const reconnectAttemptsRef = useRef(0);
const MAX_RECONNECT_ATTEMPTS = 5;
const RECONNECT_DELAY_MS = 5000; // 5 seconds between attempts
const FALLBACK_POLLING_INTERVAL = 7500; // 7.5 seconds (compromise between 5-10s)

const attemptRealtimeReconnect = useCallback((roomId: string) => {
  reconnectAttemptsRef.current += 1;
  
  if (reconnectAttemptsRef.current > MAX_RECONNECT_ATTEMPTS) {
    console.error('[room] Max reconnection attempts reached. Falling back to polling.');
    // After 5 retries, gracefully degrade to slower polling
    setUseRealtime(false);
    startPolling(roomId, FALLBACK_POLLING_INTERVAL);
    toast.warning('Using slower updates. Refresh page for better experience.');
    return;
  }
  
  console.log(`[room] Attempting real-time reconnection (${reconnectAttemptsRef.current}/${MAX_RECONNECT_ATTEMPTS})`);
  
  setTimeout(() => {
    // Clean up old channel
    if (realtimeChannelRef.current) {
      supabase.removeChannel(realtimeChannelRef.current);
      realtimeChannelRef.current = null;
    }
    
    // Retry real-time subscription
    subscribeToRoom(roomId);
  }, RECONNECT_DELAY_MS);
}, [subscribeToRoom, startPolling]);
```

**Update Polling Function:**
```typescript
const startPolling = useCallback((roomId: string, interval: number = FALLBACK_POLLING_INTERVAL) => {
  // Stop any existing polling
  if (pollingIntervalRef.current) {
    clearInterval(pollingIntervalRef.current);
    pollingIntervalRef.current = null;
  }
  
  console.log(`[room] Starting polling fallback (${interval}ms interval)`);
  
  // Initial fetch
  refreshRoomState(roomId);
  
  // Start polling with specified interval
  pollingIntervalRef.current = setInterval(() => {
    refreshRoomState(roomId);
  }, interval);
}, [refreshRoomState]);
```

**Keep (Modified):**
- Line 612: `setInterval` for room state polling → Update to use `startPolling(roomId, interval)` function
- `startPolling()` function → Update to accept custom interval parameter (7.5s fallback)

---

#### Task 1.2: Add Real-Time User Status Subscription

**File:** `src/app/room/[code]/page.tsx`

**Action:** Add subscription for approval status changes

**Add Callback:**
```typescript
const subscribeToUserStatus = useCallback((roomId: string, userId: string) => {
  // Only subscribe if user is pending
  if (userApprovalStatus !== 'pending') {
    return;
  }

  const channel = supabase
    .channel(`user-status-${roomId}-${userId}`)
    .on(
      'postgres_changes',
      {
        event: 'UPDATE',
        schema: 'public',
        table: 'kara_room_participants',
        filter: `room_id=eq.${roomId} AND user_id=eq.${userId}`
      },
      (payload) => {
        console.log('[room] Realtime: User status updated', payload);
        const newStatus = payload.new?.status;
        if (newStatus) {
          setUserApprovalStatus(newStatus);
          
          // Stop subscription if approved or denied
          if (newStatus === 'approved' || newStatus === 'denied') {
            supabase.removeChannel(channel);
          }
        }
      }
    )
    .subscribe((status) => {
      console.log('[room] User status subscription:', status);
    });

  return channel;
}, [userApprovalStatus]);
```

**Integration:**
- Call when user joins room with pending status
- Stop subscription when status changes to approved/denied

**Remove:**
- Line 645: `setInterval` for approval polling

---

#### Task 1.3: Update useEffect Hook

**File:** `src/app/room/[code]/page.tsx`

**Current:**
```typescript
useEffect(() => {
  if (roomId) {
    startPolling(roomId);
  }
  return () => {
    if (pollingIntervalRef.current) {
      clearInterval(pollingIntervalRef.current);
    }
  };
}, [roomId, startPolling]);
```

**New:**
```typescript
useEffect(() => {
  if (roomId) {
    // Reset reconnect attempts on new room
    reconnectAttemptsRef.current = 0;
    
    if (useRealtime) {
      // Try real-time first
      subscribeToRoom(roomId);
    } else {
      // Fallback to polling (slower interval)
      startPolling(roomId, FALLBACK_POLLING_INTERVAL);
    }
  }
  
  return () => {
    // Cleanup polling
    if (pollingIntervalRef.current) {
      clearInterval(pollingIntervalRef.current);
      pollingIntervalRef.current = null;
    }
    
    // Cleanup real-time
    if (realtimeChannelRef.current) {
      supabase.removeChannel(realtimeChannelRef.current);
      realtimeChannelRef.current = null;
    }
    
    // Reset reconnect attempts
    reconnectAttemptsRef.current = 0;
  };
}, [roomId, useRealtime, subscribeToRoom, startPolling]);
```

---

### Phase 2: Update TV Page Real-Time (Optimize Polling Fallback)

#### Task 2.0: Add Retry Logic to TV Page

**File:** `src/app/tv/page.tsx`

**Current Issue:** TV page immediately falls back to 2.5s polling if real-time fails (line 344)

**Action:** Add retry logic before fallback, increase fallback polling interval

**Update `subscribeToRoom` callback (line 322-332):**
```typescript
// BEFORE (line 328-331):
} else if (status === 'CHANNEL_ERROR' || status === 'TIMED_OUT') {
  console.error('[tv] ❌ Realtime failed, falling back to polling');
  setUseRealtime(false);
  startPolling(roomId); // ← Immediately polls at 2.5s
}

// AFTER:
} else if (status === 'CHANNEL_ERROR' || status === 'TIMED_OUT') {
  console.error('[tv] ❌ Realtime connection failed');
  // Try to reconnect first (5 attempts)
  attemptRealtimeReconnect(roomId);
}
```

**Add retry logic** (same as Room page Task 1.1)
- 5 reconnection attempts at 5 second intervals
- After max retries → fallback to 7.5s polling

**Update:** `startPolling()` function to use 7.5s interval instead of 2.5s

---

### Phase 3: Optimize ApprovalQueue

#### Task 3.1: Optimize ApprovalQueue Polling

**File:** `src/components/ApprovalQueue.tsx`

**Action:** Remove backup polling (line 322), increase fallback polling interval (line 333)

**Current:**
```typescript
if (useRealtime) {
  subscribeToParticipants();
  // Still poll every 10s as backup ← REMOVE THIS
  const backupInterval = setInterval(() => fetchParticipants(), 10000);
  return () => {
    clearInterval(backupInterval);
    // ...
  };
} else {
  // Fallback: poll every 3 seconds ← INCREASE TO 7.5s
  const interval = setInterval(() => fetchParticipants(), 3000);
  return () => clearInterval(interval);
}
```

**New:**
```typescript
const FALLBACK_POLLING_INTERVAL = 7500; // 7.5 seconds

if (useRealtime) {
  subscribeToParticipants();
  // No backup polling - trust real-time or fallback to slower polling
  return () => {
    if (realtimeChannelRef.current) {
      supabase.removeChannel(realtimeChannelRef.current);
      realtimeChannelRef.current = null;
    }
  };
} else {
  // Graceful degradation: slower polling
  const interval = setInterval(() => fetchParticipants(), FALLBACK_POLLING_INTERVAL);
  return () => clearInterval(interval);
}
```

**Reason:** 
- Remove backup polling: wastes 6 API calls/minute
- Keep fallback polling but slower: 7.5s instead of 3s
- Current: 3s polling = 20 calls/min
- New fallback: 7.5s polling = 8 calls/min
- Real-time working: ~0 calls/min (99% reduction)
- Real-time failed: 8 calls/min (60% reduction)

---

### Phase 4: Testing & Verification

#### Task 4.1: Functional Testing

**Test Cases:**

1. **Real-Time Connection:**
   - [ ] Page loads → real-time connects within 2s
   - [ ] Console shows "✅ Realtime connected"
   - [ ] No polling intervals in Network tab

2. **Queue Updates:**
   - [ ] User A adds song → User B sees it instantly (<500ms)
   - [ ] User A removes song → User B sees update instantly
   - [ ] Host reorders song → All users see update instantly

3. **Room State Updates:**
   - [ ] Host skips song → All users see next song instantly
   - [ ] Queue mode changes → All users see update instantly

4. **Approval Flow:**
   - [ ] User joins (pending) → Host sees notification instantly
   - [ ] Host approves → User status updates instantly
   - [ ] Host denies → User sees denial instantly

5. **Reconnection & Fallback Behavior:**
   - [ ] Simulate network disconnect → Attempts reconnection (5 retries, 5s intervals)
   - [ ] After 5 failed retries → Falls back to 7.5s polling
   - [ ] Verify fallback polling is 7.5s (NOT 2.5s) in Network tab
   - [ ] Reconnect network → Should attempt to restore real-time
   - [ ] Toast shows "Using slower updates" message during fallback

6. **Multi-User Stress Test:**
   - [ ] 5+ users in room
   - [ ] All add songs simultaneously
   - [ ] All users see all updates within 1s
   - [ ] No dropped events

---

#### Task 4.2: Performance Monitoring

**Metrics to Track:**

1. **Vercel Dashboard:**
   - [ ] Edge Function Invocations: Should drop ~95%
   - [ ] API call rate before: ~24-44/min per user
   - [ ] API call rate after: ~1 per user session
   - [ ] Monitor for 24 hours

2. **Supabase Dashboard:**
   - [ ] Realtime Concurrent Connections: Should stay <100 (well under 500 limit)
   - [ ] Realtime Messages: Should increase but stay <1% of 5M limit
   - [ ] No connection errors or timeouts

3. **Browser Performance:**
   - [ ] Network tab: Zero polling requests
   - [ ] Console: No error messages
   - [ ] Memory usage: No leaks (monitor over 30 min)
   - [ ] CPU usage: No high usage spikes

---

#### Task 4.3: Rollback Plan

**IMPORTANT:** Graceful degradation is built-in, but full rollback available if needed

**If issues occur:**

1. **Built-in Graceful Degradation:**
   - Real-time failure → Automatic 5 retry attempts
   - After retries → Automatic fallback to 7.5s polling
   - Service continues with degraded performance
   - **No action needed** - system handles it automatically

2. **Code Rollback (If Needed):**
   - Git revert to v4.7.7: `git revert HEAD`
   - Deploy previous version (with 2.5s polling)
   - Investigate issues offline

3. **Monitoring Triggers (Consider Rollback):**
   - Realtime connection success rate <80% (too many fallbacks)
   - User complaints about stale data in fallback mode
   - Supabase connection errors >10%
   - **Vercel invocations don't decrease at all** ⚠️ CRITICAL
   - Fallback polling causes performance issues

4. **Expected Behavior:**
   - ✅ Automatic polling fallback is NORMAL (not a failure)
   - ✅ Most users should stay on real-time (>90%)
   - ✅ Fallback ensures service continuity
   - ⚠️ If >20% of users on fallback, investigate infrastructure

---

## Expected Impact

### API Call Reduction

**Before (Current State):**
| Component | Frequency | Calls/Min | Endpoint |
|-----------|-----------|-----------|----------|
| Room state polling | 2.5s | 24 | `/api/rooms/[roomId]/state` |
| Approval polling | 3s | 20 | `/api/rooms/[roomId]/user-status` |
| ApprovalQueue backup | 10s | 6 | `/api/rooms/[roomId]/pending-users` |
| **Total per user** | - | **24-50** | - |

**After (Real-Time - Best Case):**
| Component | Frequency | Calls/Session | Endpoint |
|-----------|-----------|---------------|----------|
| Initial room load | Once | 1 | `/api/rooms/[roomId]/state` |
| Real-time updates | WebSocket | 0 | Supabase real-time |
| **Total per user** | - | **~1** | - |

**After (Fallback - Degraded Mode):**
| Component | Frequency | Calls/Min | Endpoint |
|-----------|-----------|-----------|----------|
| Initial room load | Once | N/A | `/api/rooms/[roomId]/state` |
| Fallback polling | 7.5s | 8 | `/api/rooms/[roomId]/state` |
| **Total per user** | - | **8** | - |

**Cost Savings:**
- **Real-time mode:** 24-50 calls/min → 1 call/session = **~99% reduction**
- **Fallback mode:** 24-50 calls/min → 8 calls/min = **67-83% reduction**
- **Expected mix:** 90% real-time, 10% fallback = **~95% overall reduction**
- **Example:** 10 active users, 9 on real-time + 1 on fallback → 9 calls (one-time) + 8 calls/min = ~72 calls/min (vs 240-500 before)

---

### User Experience Improvements

**Before:**
- Updates appear every 2.5 seconds
- Noticeable delay when adding/removing songs
- Approval status checks every 3 seconds

**After (Real-Time Mode - 90% of users):**
- Updates appear instantly (<500ms)
- Real-time queue changes
- Instant approval notifications
- Significantly better perceived performance

**After (Fallback Mode - 10% of users):**
- Updates appear every 7.5 seconds
- Slightly slower than before (7.5s vs 2.5s)
- Still functional and usable
- Toast notification explains slower updates
- User can refresh page to retry real-time

**Overall:**
- Most users get instant updates
- Service remains available for everyone
- Clear communication when in degraded mode

---

### Infrastructure Impact

**Vercel:**
- ✅ 95-99% reduction in Edge Function invocations
- ✅ Lower bandwidth usage
- ✅ Reduced costs

**Supabase:**
- ⚠️ Increase in real-time connections (3 → ~50-100)
- ⚠️ Increase in real-time messages (~700 → ~10k/day)
- ✅ Still well within free tier limits (500 connections, 5M messages)

---

## Success Criteria

### Must Have (Blockers)
- [ ] Zero polling intervals when real-time is connected
- [ ] Real-time attempts 5 reconnections before falling back
- [ ] Fallback polling uses 7.5s interval (NOT 2.5s)
- [ ] All queue/room updates appear within 500ms (real-time mode)
- [ ] All queue/room updates appear within 8s (fallback mode)
- [ ] Approval status updates instantly for pending users (real-time mode)
- [ ] Network tab shows 1 initial API call when real-time works
- [ ] Network tab shows 8 calls/min max when fallback polling active
- [ ] Vercel invocations drop by 90-99% overall
- [ ] Real-time connection success rate >90%
- [ ] No functional regressions from v4.7.7
- [ ] Service remains available even if real-time fails

### Nice to Have (Enhancements)
- [ ] Connection status indicator in UI
- [ ] Reconnection toast notification
- [ ] Real-time health metrics dashboard
- [ ] Automatic reconnection on network restore

---

## Risk Assessment

### High Risk (Mitigated)
1. **Real-time connection failures**
   - **Mitigation:** Automatic fallback to polling (proven in TV page)
   - **Test:** Simulate network failures

2. **Missed events during reconnection**
   - **Mitigation:** Full state refresh after reconnection
   - **Test:** Disconnect/reconnect stress test

### Medium Risk (Monitored)
1. **Supabase connection limits**
   - **Current:** 3/500 connections
   - **Expected:** <100/500 connections
   - **Monitoring:** Dashboard alerts if >400

2. **Message rate limits**
   - **Current:** 690/5M messages
   - **Expected:** <100k/5M messages
   - **Monitoring:** Dashboard alerts if >4M

### Low Risk
1. **Increased memory usage (WebSocket connections)**
   - **Impact:** Minimal (~5KB per connection)
   - **Browser limit:** 6 connections per domain (we use 1-2)

---

## Implementation Timeline

### Pre-Implementation
- [x] Complete codebase review
- [x] Document current state
- [x] Create detailed plan
- [x] Tag v4.7.7
- [x] Commit plan to git

### Phase 1: Room Page Migration (Estimated: 2-4 hours)
- [ ] Verify NO CACHING in api.ts
- [ ] Add real-time state management
- [ ] Add room/queue subscription with retry logic
- [ ] Add user status subscription
- [ ] REMOVE all polling code completely
- [ ] Test locally

### Phase 2: TV Page Update (Estimated: 1 hour)
- [ ] Replace polling fallback with retry logic
- [ ] REMOVE startPolling() function
- [ ] Test locally

### Phase 3: ApprovalQueue Optimization (Estimated: 30 min)
- [ ] Remove backup polling (10s)
- [ ] Remove fallback polling (3s)
- [ ] Test locally

### Phase 4: Testing (Estimated: 2-3 hours)
- [ ] Functional testing (all test cases)
- [ ] Verify ZERO polling in Network tab
- [ ] Performance monitoring (24 hours)
- [ ] Multi-user stress test
- [ ] Verify Vercel metrics DROP (not increase)

### Phase 5: Deployment (Estimated: 1 hour)
- [ ] Deploy to production
- [ ] Monitor Vercel dashboard (invocations must decrease)
- [ ] Monitor Supabase dashboard
- [ ] Verify user experience
- [ ] Ready to rollback to v4.7.7 if needed

---

## Rollback Strategy

**Trigger Conditions:**
- Realtime connection failure rate >10%
- API calls don't decrease by >90%
- User reports of stale data
- Console errors in production

**Rollback Steps:**
1. Revert git to v4.7.7: `git revert HEAD`
2. Deploy previous version
3. Verify polling restored
4. Investigate issues offline

**Communication:**
- Notify users of temporary revert
- Provide ETA for fix
- Document issues for next attempt

---

## Post-Implementation

### Monitoring (First 7 Days)
- [ ] Daily check of Vercel invocations
- [ ] Daily check of Supabase connections
- [ ] Monitor error logs
- [ ] Collect user feedback

### Documentation Updates
- [ ] Update README with real-time info
- [ ] Document troubleshooting steps
- [ ] Create real-time architecture diagram
- [ ] Add to release notes

### Future Enhancements
- [ ] Add connection status UI indicator
- [ ] Implement reconnection retry logic
- [ ] Add real-time analytics dashboard
- [ ] Optimize message batching

---

## Technical Reference

### Supabase Real-Time API

**Channel Creation:**
```typescript
const channel = supabase.channel('channel-name')
```

**Subscribe to Table Changes:**
```typescript
.on('postgres_changes', {
  event: 'INSERT' | 'UPDATE' | 'DELETE' | '*',
  schema: 'public',
  table: 'table_name',
  filter: 'column=eq.value'
}, (payload) => {
  // Handle change
})
```

**Subscribe & Handle Status:**
```typescript
.subscribe((status) => {
  if (status === 'SUBSCRIBED') {
    // Connected
  } else if (status === 'CHANNEL_ERROR' || status === 'TIMED_OUT') {
    // Fallback to polling
  }
})
```

**Cleanup:**
```typescript
supabase.removeChannel(channel)
```

---

### Tables to Subscribe

**kara_rooms:**
- **Events:** UPDATE
- **Fields:** current_entry_id, queue_mode, is_active
- **Filter:** `id=eq.{roomId}`

**kara_queue:**
- **Events:** INSERT, UPDATE, DELETE (*)
- **Fields:** status, position, sort_key, round_number
- **Filter:** `room_id=eq.{roomId}`

**kara_room_participants:**
- **Events:** UPDATE
- **Fields:** status (pending, approved, denied)
- **Filter:** `room_id=eq.{roomId} AND user_id=eq.{userId}`

---

## Appendix

### Related Files
- `src/app/tv/page.tsx` - Reference implementation (lines 280-335)
- `src/app/room/[code]/page.tsx` - Target for migration
- `src/components/ApprovalQueue.tsx` - Backup polling removal
- `src/lib/api.ts` - API client (no changes needed)
- `src/lib/supabase.ts` - Supabase client (no changes needed)

### Git History
- **v4.7.7** - Current version (pre-migration)
- **v4.8.0** - This migration (planned)

### References
- [Supabase Realtime Docs](https://supabase.com/docs/guides/realtime)
- [TV Page Real-Time PR](https://github.com/...) - Original implementation
- [Vercel Function Pricing](https://vercel.com/pricing)
- [Supabase Pricing](https://supabase.com/pricing)

---

**Document Version:** 1.2  
**Last Updated:** 2026-01-21 (Updated)  
**Author:** AI Assistant  
**Status:** Planning Phase - Ready for Implementation

**v1.2 Changes (FINAL):**
- ✅ Changed approach: Real-time first with graceful degradation
- ✅ Added 5 reconnection attempts (5s intervals) before fallback
- ✅ Fallback to slower polling (7.5s) instead of complete failure
- ✅ Service continuity prioritized - users always have working app
- ✅ Updated all code examples with fallback logic
- ✅ Updated success criteria: 90-99% reduction (not all-or-nothing)
- ✅ Updated test cases to verify graceful degradation
- ✅ Expected: 90% users on real-time, 10% on fallback
- ✅ Overall target: ~95% reduction in API calls

**Previous Versions:**
- v1.1: All-or-nothing approach (rejected - too risky for users)
- v1.0: Initial plan with immediate polling fallback
